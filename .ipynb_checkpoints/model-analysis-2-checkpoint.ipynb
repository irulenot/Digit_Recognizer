{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "#### Model Performance Accuracy (normalization + feature selection + uniform dataset)\n",
    "    Linear Regression: 22.89%\n",
    "    Logistic Regression: 89.93%\n",
    "    SVM: 93.41%  \n",
    "    NN: 96.25%\n",
    "    KNN: Too many features for my device to compute\n",
    "    \n",
    "#### Kaggle Results\n",
    "    SVM: 91.22%\n",
    "    NN (original dataset): 94.81%\n",
    "        (normalization): 96.58%\n",
    "        (feature selection): 93.85%\n",
    "        (normalization + feature selection): ?\n",
    "    NN (uniform dataset): ?\n",
    "        (normalization): _\n",
    "        (normalization + feature selection): 95.18% \n",
    "    \n",
    "#### Findings\n",
    "    Normalization improves accuracy by ~2%\n",
    "    Feature selection improves efficiency by ~10% but decreases accuracy by ~1%\n",
    "    Optimization via shuffling the data can increase accuracy by ~1%\n",
    "    \n",
    "    TO DO: The uniform training set with only normalization will be the best\n",
    "    \n",
    "#### Hyperparameters for normalized NN\n",
    "    Hidden Layer Activation: relu\n",
    "    Solver: adam\n",
    "    Alpha: 0.0001\n",
    "    Learning rate: 0.001\n",
    "    Shuffle: True\n",
    "    Tolerence for Optimization : 0.0001\n",
    "    Gradient Descent: 0.9\n",
    "    Validation fraction: 0.1\n",
    "    Loss fcn: 0.004860026848415791\n",
    "    Iterations: 105\n",
    "    Layers: 3 (1 hidden layer)\n",
    "    Output activation: softmax\n",
    "\n",
    "#### Notes\n",
    "    By Uniform dataset I mean the labels in the dataset are a distributed uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "df_train = pd.read_csv(path+'train_data.csv')\n",
    "df_val = pd.read_csv(path+'val_data.csv')\n",
    "df_test = pd.read_csv(path+'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframes -> numpy arrays to train models off of \n",
    "train = df_train.values\n",
    "val = df_val.values\n",
    "test = df_test.values\n",
    "\n",
    "Xtr = train[:,1:]\n",
    "Ytr = train[:,0]\n",
    "Xval = val[:,1:]\n",
    "Yval = val[:,0]\n",
    "Xte = test[:,1:]\n",
    "Yte = test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data to use simple Keras models\n",
    "from sklearn import preprocessing\n",
    "Xtr = preprocessing.normalize(Xtr)\n",
    "Xval = preprocessing.normalize(Xval)\n",
    "Xte = preprocessing.normalize(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model accuracy: 24.44%\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression with feature selection and normalization\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Xtr, Ytr)\n",
    "Yhat = np.floor(regr.predict(Xval))\n",
    "print(\"Linear model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model accuracy: 89.93%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with feature selection and normalization and uniform dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtr, Ytr)\n",
    "Yhat = lr.predict(Xval)\n",
    "print(\"Linear model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model accuracy: 93.41%\n"
     ]
    }
   ],
   "source": [
    "# SVM with feature selection and normalization and uniform dataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "Yhat = OneVsOneClassifier(LinearSVC(random_state=0)).fit(Xtr, Ytr).predict(Xval)\n",
    "print(\"SVM model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model accuracy: 96.17%\n"
     ]
    }
   ],
   "source": [
    "# NN with feature selection and normalization and uniform dataset\n",
    "# Takes much longer than SVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(Xtr, Ytr)\n",
    "Yhat = clf.predict(Xval)\n",
    "print(\"NN model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN takes too long on my device due to all the features present in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and feature selection and uniform dataset\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "# Importing modified training data\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'model_train.csv')\n",
    "\n",
    "# Removing all features from test.csv not evaluated in our model\n",
    "for i in range(784):\n",
    "    col = \"pixel\" + str(i)\n",
    "    if col not in train_model.columns:\n",
    "        test_model = test_model.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle SVM with normalization and feature selection and uniform dataset\n",
    "\n",
    "Yhat = OneVsOneClassifier(LinearSVC(random_state=0)).fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "# Creating submission file\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i') # I add: \"ImageId,Label\" manually to first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with normalization and feature selection and uniform dataset\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with feature selection on original dataset\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "\n",
    "# Removing all features from test.csv not evaluated in our model\n",
    "for i in range(784):\n",
    "    col = \"pixel\" + str(i)\n",
    "    if col not in df_train.columns:\n",
    "        train_model = train_model.drop([col], axis=1)\n",
    "        \n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN on original data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with normalized data on original data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "labels = train_model.label.values\n",
    "train_model = preprocessing.normalize(train_model.iloc[:,1:])\n",
    "\n",
    "model = MLPClassifier().fit(train_model, labels)\n",
    "Yhat = model.predict(test_model.values)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i') # Note: Dataframes run quicker than numpy arrays on scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu\n",
      "adam\n",
      "0.0001\n",
      "0.001\n",
      "True\n",
      "0.0001\n",
      "0.9\n",
      "0.1\n",
      "0.004860026848415791\n",
      "105\n",
      "3\n",
      "softmax\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of NN above\n",
    "\n",
    "print(model.activation)\n",
    "print(model.solver)\n",
    "print(model.alpha)\n",
    "print(model.learning_rate_init)\n",
    "print(model.shuffle)\n",
    "print(model.tol)\n",
    "print(model.momentum)\n",
    "print(model.validation_fraction)\n",
    "print(model.loss_)\n",
    "print(model.n_iter_)\n",
    "print(model.n_layers_)\n",
    "print(model.out_activation_)\n",
    "\n",
    "# Other hyperparameters\n",
    "# print(model.batch_size) # batch_size=min(200, n_samples) # print(model.learning_rate) # print(model.power_t)\n",
    "# print(model.max_iter) # print(model.random_state) # print(model.verbose) # print(model.warm_start)\n",
    "# print(model.nesterovs_momentum) # print(model.early_stopping) # print(model.beta_1) # print(model.beta_2)\n",
    "# print(model.epsilon) # print(model.classes_) # print(model.coefs_) # print(model.intercepts_) # print(model.n_outputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with normalized data on uniform data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'uniform.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "labels = train_model.label.values\n",
    "train_model = preprocessing.normalize(train_model.iloc[:,1:])\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model, labels).predict(test_model.values)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i') # Note: Dataframes run quicker than numpy arrays on scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN on uniform data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat)) ******************TODO\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN on uniform data with feature selection\n",
    "********** TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN on original data with feature selection and normalization\n",
    "********* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model accuracy: 93.64%\n",
      "NN model accuracy: 94.28%\n"
     ]
    }
   ],
   "source": [
    "# Testing effects of shuffling data\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(Xtr, Ytr)\n",
    "Yhat = clf.predict(Xval)\n",
    "print(\"NN model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))\n",
    "\n",
    "np.random.shuffle(train)\n",
    "Xtr = train[:,1:]\n",
    "Ytr = train[:,0]\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(Xtr, Ytr)\n",
    "Yhat = clf.predict(Xval)\n",
    "print(\"NN model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
