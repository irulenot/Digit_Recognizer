{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "#### Model Performance Accuracy (normalization + feature selection + uniform dataset)\n",
    "    Linear Regression: 22.89%\n",
    "    Logistic Regression: 89.93%\n",
    "    SVM: 93.41%  \n",
    "    NN: 96.25%\n",
    "    KNN: Too many features for my device\n",
    "    \n",
    "#### Kaggle Results\n",
    "    SVM: 91.22%\n",
    "    KAGGLE: (normalization): 96.58%\n",
    "            (normalization + feature selection): 95.18% \n",
    "            (original): 94.81%\n",
    "            (feature selection): 93.85%\n",
    "    \n",
    "#### Results\n",
    "    Normalization improves accuracy by ~2%\n",
    "    Feature selection improves efficiency by ~10% but decreases accuracy by ~1%\n",
    "    Hypothesis: The uniform training set improves accuracy by _%  \n",
    "    \n",
    "#### Hyperparameters\n",
    "\n",
    "\n",
    "*By Uniform dataset I mean there are a uniform amount of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "df_train = pd.read_csv(path+'train_data.csv')\n",
    "df_val = pd.read_csv(path+'val_data.csv')\n",
    "df_test = pd.read_csv(path+'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframes -> numpy arrays to train models off of \n",
    "train = df_train.values\n",
    "val = df_val.values\n",
    "test = df_test.values\n",
    "\n",
    "Xtr = train[:,1:]\n",
    "Ytr = train[:,0]\n",
    "Xval = val[:,1:]\n",
    "Yval = val[:,0]\n",
    "Xte = test[:,1:]\n",
    "Yte = test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data to use simple Keras models\n",
    "from sklearn import preprocessing\n",
    "Xtr = preprocessing.normalize(Xtr)\n",
    "Xval = preprocessing.normalize(Xval)\n",
    "Xte = preprocessing.normalize(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model accuracy: 24.44%\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression with feature selection and normalization\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Xtr, Ytr)\n",
    "Yhat = np.floor(regr.predict(Xval))\n",
    "print(\"Linear model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model accuracy: 89.93%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with feature selection and normalization and uniform dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtr, Ytr)\n",
    "Yhat = lr.predict(Xval)\n",
    "print(\"Linear model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model accuracy: 93.41%\n"
     ]
    }
   ],
   "source": [
    "# SVM with feature selection and normalization and uniform dataset\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "Yhat = OneVsOneClassifier(LinearSVC(random_state=0)).fit(Xtr, Ytr).predict(Xval)\n",
    "print(\"SVM model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model accuracy: 96.17%\n"
     ]
    }
   ],
   "source": [
    "# NN with feature selection and normalization and uniform dataset\n",
    "# Takes much longer than SVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(Xtr, Ytr)\n",
    "Yhat = clf.predict(Xval)\n",
    "print(\"NN model accuracy: {0:.2f}%\".format(accuracy_score(Yval, Yhat)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN takes too long on my device due to all the features present in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and feature selection and uniform dataset\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "# Importing modified training data\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'model_train.csv')\n",
    "\n",
    "# Removing all features from test.csv not evaluated in our model\n",
    "for i in range(784):\n",
    "    col = \"pixel\" + str(i)\n",
    "    if col not in train_model.columns:\n",
    "        test_model = test_model.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle SVM with normalization and feature selection and uniform dataset\n",
    "\n",
    "Yhat = OneVsOneClassifier(LinearSVC(random_state=0)).fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "# Creating submission file\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i') # I add: \"ImageId,Label\" manually to first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with normalization and feature selection and uniform dataset\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with feature selection on original set and uniform dataset\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "\n",
    "# Removing all features from test.csv not evaluated in our model\n",
    "for i in range(784):\n",
    "    col = \"pixel\" + str(i)\n",
    "    if col not in df_train.columns:\n",
    "        train_model = train_model.drop([col], axis=1)\n",
    "        \n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN on original data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model.iloc[:,1:], train_model.iloc[:,0]).predict(test_model)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle NN with normalized data on original data\n",
    "\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "train_model = pd.read_csv(path+'train.csv')\n",
    "path = '/Users/antonmax2/Documents/dev/data/digit_recognizer/'\n",
    "test_model = pd.read_csv(path+'test.csv')\n",
    "\n",
    "labels = train_model.label.values\n",
    "train_model = preprocessing.normalize(train_model.iloc[:,1:])\n",
    "\n",
    "Yhat = MLPClassifier().fit(train_model, labels).predict(test_model.values)\n",
    "\n",
    "index = np.arange(1, Yhat.shape[0]+1)\n",
    "submission = np.column_stack((index,Yhat))\n",
    "np.savetxt('submission.txt', submission, delimiter=',', fmt='%i') # Note: Dataframes run quicker than numpy arrays on scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (28000,784) and (708,100) not aligned: 784 (dim 1) != 708 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-7f416f10adc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mYhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 105\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (28000,784) and (708,100) not aligned: 784 (dim 1) != 708 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Kaggle NN with normalized data on uniform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train model on raw train set, and on non uniform set, and on pixels included"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
